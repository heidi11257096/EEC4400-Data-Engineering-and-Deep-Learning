# EEC4400 作业（25/26学年第1学期）
## 1 探索强化学习：用于倒立摆（Cart-Pole）任务的Q学习、朴素DQN、DQN与DDQN算法
强化学习（Reinforcement Learning, RL）是机器学习的核心分支之一，其核心机制是智能体（agent）通过与环境交互，学习如何做出序列决策。与依赖带标签数据的监督学习不同，强化学习智能体通过“试错法”提升性能——根据自身行动获得相应奖励，进而优化决策策略。在测试强化学习算法时，**倒立摆（Cart-Pole）** 是广泛使用的经典控制环境：该任务要求通过对小车施加向左或向右的力，使小车上方的杆子保持平衡。

在深度强化学习（Deep Reinforcement Learning, DRL）中，神经网络（Neural Networks, NNs）被用于近似或存储核心组件（例如Q学习中的Q值）。本作业要求学生实现并探索四种深度强化学习方法（以下统称“DRL算法”），以解决倒立摆环境任务：
1. **基于神经网络的Q学习（Q-Network）**：Q学习的直接应用，使用神经网络近似Q值，不包含经验回放缓冲区或目标网络。
2. **朴素DQN（单网络）**：一种基准方法，仅使用单个神经网络同时完成动作选择与动作评估。
3. **DQN（含目标网络）**：标准DQN方法（参考论文：[https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)），其核心特点是通过周期性更新的“目标网络”提升训练稳定性。
4. **双重DQN（Double DQN, DDQN）**：一种改进方法，通过将“动作选择”与“动作评估”解耦，解决Q值高估偏差问题（参考论文：[https://arxiv.org/abs/1509.06461](https://arxiv.org/abs/1509.06461)）。

通过本作业，学生将获得以下实践经验：实现不同的深度强化学习技术、对比各算法的优势与局限性、理解不同超参数如何影响DRL智能体在序列决策任务中的性能。


## 2 开发平台
为简化实现流程并统一评分标准，要求学生使用**Jupyter笔记本（Jupyter Notebook）** 实现上述DRL算法。作业将提供一个“框架代码笔记本”，学生需严格按照该笔记本中的指导完成代码编写。框架代码已实现部分核心功能，学生需在此基础上补充代码以完成完整任务。


## 3 程序实现
本节将概述框架笔记本的结构，并明确学生需自主实现的部分。

> **注意**：本作业**禁止使用**强化学习专用库（如Stable Baselines、RLlib等）。


### 3.1 背景知识——[完整代码已提供]
本节将介绍倒立摆环境，并引导学生熟悉Gymnasium（强化学习环境库）的基础操作。在实现DRL算法前，学生需仔细研读本节代码，充分了解环境机制。


### 3.2 配置TensorBoard——[完整代码已提供]
本节需创建日志目录（`eec4400_logs`），用于存储各算法的训练过程数据，以便后续通过TensorBoard可视化训练动态。


### 3.3 DRL智能体的训练与评估
每位学生需负责实现并训练以下四种DRL算法中的一种，具体分配如下：
- Q-Network：学生1
- 朴素DQN：学生2
- DQN：学生3
- DDQN：学生4

所有DRL算法的训练与评估需遵循以下通用流程：
1. 初始化基准超参数（详见3.5节）。
2. 构建神经网络 + [**需编写代码**]。
3. 实现、训练并评估四种DRL智能体（详见3.4节） + [**需编写代码**]。
4. 集成TensorBoard，跟踪训练进度。
5. 计算并绘制训练奖励的移动平均值（窗口大小`window_size = 20`） + [学生1：需编写代码]。
6. 计算并绘制每轮训练的评估奖励均值与方差 + [学生1：需编写代码]。
7. 跟踪并报告每轮训练的平均耗时。
8. 使用备选超参数（详见3.5节），重复步骤2、3、5、6、7。

> **协作要求**：四名学生需共同讨论并理解四种DRL算法的异同，确保代码实现中“程序语句流程”“变量使用”“数据结构设计”的一致性与规范性。


### 3.4 DRL策略的性能评估
每轮训练结束后，需对当前DRL策略进行评估：使用该策略生成若干轮动作，记录获得奖励的均值与方差。DRL智能体的性能可通过以下指标反映：
- 每轮训练的训练奖励
- 每轮训练的评估奖励均值
- 每轮训练的评估奖励方差
- 每轮训练的平均耗时（训练结束后统一计算）

学生需绘制以下指标随“训练轮次（episodes）”变化的曲线：
- 训练奖励的移动平均值
- 评估奖励均值
- 评估奖励方差


### 3.5 超参数调优
超参数调优是优化模型性能与稳定性的关键步骤。在深度强化学习中，超参数可分为两类：
- **神经网络超参数（NN hyperparameters）**：控制神经网络的性能（含网络结构）。
- **强化学习超参数（RL hyperparameters）**：在神经网络结构固定的前提下，控制DRL智能体的性能。

#### 超参数调优核心要求：
1. 识别关键超参数，并将其分类为“神经网络超参数”或“强化学习超参数”（需在报告中记录，详见4节）。
2. 小组共同确定：
   - 基准神经网络超参数集（记为`NN-hp-set1`）。
   - 基准强化学习超参数集（记为`RL-hp-set1`）。
3. 每位学生使用`NN-hp-set1`与`RL-hp-set1`，在自己负责的DRL算法上运行实验，得到**基准策略**。
4. 评估基准策略（详见3.4节），并通过TensorBoard分析神经网络的内部权重（详见3.6节）。
5. 小组共同设计：
   - 备选神经网络超参数集（记为`NN-hp-set2`）。
   - 备选强化学习超参数集（记为`RL-hp-set2`）——目标是提升基准策略的性能。
   - （需在报告中说明设计思路，详见4节）。
6. 每位学生使用`NN-hp-set2`与`RL-hp-set2`，在自己负责的DRL算法上运行实验，得到**备选策略**。
7. 评估备选策略（详见3.4节），并通过TensorBoard分析神经网络的内部权重（详见3.6节）。

> **说明1**：实际场景中，超参数调优通常采用程序化方法（如随机搜索、网格搜索、序列模型优化SMBO等）；为简化操作，本作业采用“一步手动调优”。  
> **说明2**：为保证四种DRL算法的策略对比公平性，所有算法需使用完全相同的“神经网络/强化学习超参数值”与“神经网络结构”。


### 3.6 绘制不同DRL算法的奖励曲线
针对四种DRL算法（Q-Network、朴素DQN、DQN、DDQN），选取每种算法的“更优策略”（基准策略或备选策略），在同一张图中绘制“训练奖励移动平均值随训练轮次变化”的曲线。


### 3.7 基于TensorBoard的可视化——[完整代码已提供]
训练与评估所有DRL算法/模型后，可通过TensorBoard查看收集到的统计数据与神经网络详情。训练智能体时使用的“回调函数”（框架代码中有详细说明）会将训练过程数据存储到日志目录，这些数据可通过TensorBoard可视化。

> **注意**：不同DRL算法生成的策略，在TensorBoard中显示的结果会存在差异。


### 3.8 额外探索（选做）
每位学生可针对自己负责的DRL算法，进行额外的探索或优化。相关代码需放在Jupyter笔记本的“个人专属章节”，并在报告的“个人专属章节”中说明。


## 4 结果与报告撰写
通过上述实验，每种DRL算法将生成两个版本的策略：
- **基准DRL策略**：使用`NN-hp-set1`与`RL-hp-set1`训练得到。
- **备选DRL策略**：使用`NN-hp-set2`与`RL-hp-set2`训练得到。

学生需收集所有必要的图表（含TensorBoard可视化结果）与性能指标。


### 4.1 单一DRL算法的性能评估
#### 4.1.1 不同神经网络结构与超参数的对比
1. 以表格形式呈现`NN-hp-set1`与`NN-hp-set2`的具体数值。
2. 说明设计思路：
   - 神经网络结构的设计依据。
   - 基准与备选神经网络中各超参数值的选择理由。
   - 分析备选神经网络是否达到预期优化效果。
3. 对比基准与备选神经网络：
   - 利用TensorBoard统计数据，分析“损失（loss）/误差随迭代次数变化”的规律。
   - 观察并分析神经网络不同层的内部参数（或权重）的数值大小。

#### 4.1.2 不同强化学习超参数的对比
1. 以表格形式呈现`RL-hp-set1`与`RL-hp-set2`的具体数值。
2. 说明设计思路：基准与备选强化学习超参数值的选择理由，并分析备选策略是否达到预期优化效果。
3. 利用TensorBoard统计数据，分析神经网络中存储的“学到的策略”。
4. 手动输入特定观测值（observation）到神经网络，观察并分析输出的Q值。
5. 在报告中包含：
   - 3.4节中绘制的“训练奖励移动平均值、评估奖励均值、评估奖励方差随轮次变化”的曲线。
   - 基准与备选策略的“每轮平均训练耗时”。
   - 基于上述图表与数据，分析实验观察结果。


### 4.2 不同DRL算法的性能对比
#### 4.2.1 不同DRL算法的性能差异
1. 在报告中插入3.6节绘制的图表（四种算法的更优策略的“训练奖励移动平均值随轮次变化”曲线）。
2. 从以下维度对比四种算法的性能：
   - 在倒立摆任务中的有效性（能否完成平衡目标）。
   - 收敛速度（多久达到稳定性能）。
   - 训练稳定性（奖励波动程度）。
3. 解释算法性能差异的原因，重点分析：
   - 经验回放（Q-Network vs 朴素DQN）对性能的影响。
   - 目标网络（朴素DQN vs DQN）对性能的影响。
   - 动作选择解耦（DQN vs DDQN）对性能的影响。

#### 4.2.2 不同DRL算法的每轮平均训练耗时
1. 以表格形式呈现4.2.1节中四种“更优策略”的“每轮平均训练耗时”，并进行对比。
2. 回答以下问题：
   - “每轮平均训练耗时”是否是评估DRL算法性能的合理指标？说明理由。
   - 哪种算法的计算成本最高？其性能提升是否值得对应的计算成本增加？说明理由。


### 4.3 报告撰写要求
报告需控制在**10页以内**，重点突出以下内容（完整得分需包含所有项）：
- 神经网络结构与DRL智能体的设计依据。
- 4.1节与4.2节中所有问题的答案（回答需简洁，需用实验结果与图表支撑观点）。
- 额外探索（选做）的成果（需严格遵守页数限制）。
- 报告末尾需包含“贡献声明（Statement of Contributions）”。

> **注意事项**：
> - 若引用参考资料，需标注来源——参考文献列表不计入10页限制。
> - 报告无需过度描述代码细节，重点放在“结果分析”与“观察解释”。
> - 无实验结果支撑的推理视为无效。


## 5 职责分工
下表明确了每位学生与小组需完成的任务：

| 任务 | 学生1 | 学生2 | 学生3 | 学生4 |
|------|-------|-------|-------|-------|
| **Jupyter笔记本** | | | | |
| - 构建Q-Network智能体并评估性能 | ✓ | | | |
| - 构建朴素DQN智能体并评估性能 | | ✓ | | |
| - 构建DQN智能体并评估性能 | | | ✓ | |
| - 构建DDQN智能体并评估性能 | | | | ✓ |
| - 为四种智能体设计2组超参数 # | 全体 | 全体 | 全体 | 全体 |
| - 实现移动平均与其他绘图函数 | ✓ | | | |
| - 绘制四种DRL智能体的“奖励-轮次”对比曲线 | 全体 | 全体 | 全体 | 全体 |
| - 基于TensorBoard的可视化 | ✓ | ✓ | ✓ | ✓ |
| **报告** | | | | |
| - 4.1.1节 #、4.1.2节 # | ✓ | ✓ | ✓ | ✓ |
| - 4.2.1节、4.2.2节 | 全体 | 全体 | 全体 | 全体 |
| - 神经网络结构与DRL智能体的设计依据 # | 全体 | 全体 | 全体 | 全体 |

- `全体`：需小组协作完成的任务。
- `✓`：需个人独立完成的任务。
- `#`：关键任务，需详细记录与说明。

> **额外说明**：学生可编写额外代码收集更多实验结果，以支撑报告分析。若包含此类代码，需按负责人分类整理，并明确标注每位学生的工作内容。


## 6 提交要求与评分标准
### 6.1 提交要求
小组需在**2025年11月15日（周六）23:59**前提交以下文件：
1. 完整的Jupyter笔记本（文件名格式：`EEC4400-Groupxx.ipynb`，其中`xx`为小组编号）——需包含所有代码、输出结果与实验数据。
2. 报告（PDF格式，文件名格式：`EEC4400-Groupxx-Report.pdf`）。

将上述两个文件压缩为一个压缩包（文件名格式：`EEC4400-Groupxx.zip`），由小组统一提交（**每组仅需提交1次**）。

> **注意**：提交平台的具体操作指南将在截止日期前另行通知。


### 6.2 评分标准
- **DDQN学习要求**：DDQN未在课堂讲授，学生需自主研究相关知识。
- **个人评分独立性**：同一小组内学生的得分可能不同。个人任务的评分独立进行（例如，学生1的Q-Network实现得分不影响学生2的朴素DQN得分）。
- **实现合理性**：DRL智能体的构建需考虑实际问题的解决逻辑，学生需结合课堂所学知识与（必要时的）自主研究，说明设计选择的合理性。
- **性能权重**：DRL智能体的性能并非评分核心，但至少需有部分智能体达到合理性能水平。
- **额外探索加分**：针对个人负责的算法进行额外探索或优化，可获得额外加分。
- **报告重点**：报告无需过度总结代码，需聚焦“结果分析”与“观察解释”。