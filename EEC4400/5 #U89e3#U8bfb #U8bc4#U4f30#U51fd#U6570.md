```
def evaluation(model, max_timesteps=500):
    eval_env = gym.make("CartPole-v1")
    state_size = eval_env.observation_space.shape[0] # Number of observations (CartPole)
    action_size = eval_env.action_space.n            # Number of possible actions
    eval_reward = []

    # 初始化环境，获取状态和动作空间维度

    for i in range (5):
        round_reward = 0
        state, _ = eval_env.reset()
        state = np.reshape(state, [1, state_size])
        # 重置环境并初始化

        for i in range(max_timesteps):
            action = np.argmax(model.predict(state, verbose=0)[0])     
            # model.predict预测当前状态下作用动作的q值，取第一个样本，返回[[q_value_action0, q_value_action1]]
            # argmax选择最大的动作索引          
            #verbose=0 to 用来设置日志静默模式(其他可选值为1进度条模式，2精简日志模式)                                                                                    
            next_state, reward, terminated, truncated, _ = eval_env.step(action) 
            # 执行动作并获取反馈
            next_state = np.reshape(next_state, [1, state_size])

            round_reward += reward # 累积奖励
            state = next_state

            if terminated or truncated: # 检查是否结束
                eval_reward.append(round_reward)
                break

    eval_env.close()

    eval_reward_mean = np.sum(eval_reward)/len(eval_reward) # 平均奖励，五个回合的平均值
    eval_reward_var = np.var(eval_reward) # 奖励方差，反应策略稳定性

    return eval_reward_mean, eval_reward_var
```